{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3\n",
    "# Extremal bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of hashing, take 2\n",
    "\n",
    "Suppose we have a hash table of size $n$, where we have inserted $n$ items into the $n$ locations at random. Recall that in order to search for an item $x$, we compute $h(x)$ and look at all of the items in location $h(x)$. This is commonly known as a balls-in-bins problem, and we will refer to the items as balls and the hash table locations as bins.\n",
    "\n",
    "We can ask a number of questions:\n",
    "\n",
    "- What is the average size of a bin?\n",
    "- What is the expected value of the size of a bin?\n",
    "- Can you say something of the form: bin $i$ has size at most $x$ with probability at most $p$?\n",
    "- Can you say something of the form: all bins have size at most $x$ with probability at most $p$\n",
    "\n",
    "\n",
    "## Average\n",
    "\n",
    "We will be using the language of probability. Let $b_i$ denote the number of balls in bin $i$. The variable $b_i$ is not fixed, we use $Pr[b_i=x]$ to indicate the probability that after throwing $n$ balls into $n$ bins, that bin $i$ has size $x$. For example, $Pr[b_i=0]=(1-\\frac{1}{n})^n$, which as we learned last class, converges to $\\frac{1}{e}=37\\%$.\n",
    "\n",
    "The first question \"What is the average size of a bin\", is easy. Using the formula for average, and using the fact that the total number of balls is $n$:\n",
    "\n",
    "$$ \\text{Average bin size}= \\frac{1}{n}\\sum_{i=1}^n b_i =\\frac{1}{n}\\cdot n =1 $$\n",
    "\n",
    "This statement has nothing to do with probability. No matter how you put the balls into bins there is an average size of 1. The average size does not distinguish between one ball in each bin and all the balls in one bin. As we care about this difference, this shows that average is the wrong measure.\n",
    "\n",
    "## Expected value\n",
    "\n",
    "The second question is \"What is the expected value of the size of a bin?\" For this we need the definition of expected value:\n",
    "\n",
    "$$ E[X] = \\sum_i i Pr[X=i]$$ \n",
    "\n",
    "So with this definition in hand:\n",
    "\n",
    "$$ E[b_i] = \\sum_{i=0}^n Pr[X=i] $$\n",
    "\n",
    "What is $Pr[X=i]$? This is ${n \\choose i}(1-\\frac{1}{n})^i(\\frac{1}{n})^{n-i}$. But there is a better way.\n",
    "\n",
    "The trick is to use something known as linearity of expectation, which says:\n",
    "\n",
    "$$ E[X+Y]=E[X]+E[Y]$$\n",
    "\n",
    "But $E[b_i]$ is not a sum. Well, in fact it is. Consider the following definition:\n",
    "\n",
    "$$ b_{i,j} = \\begin{cases} 1 & \\text{if ball $j$ lands in bin $i$}\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "The random variable $b_{i,j}$ is known as an *indicator random variable*, which simply means it is either 0 or 1 depending on whether some event occurs. What is $E[b_{i,j}]$? Well this is just the probability that the event *ball $j$ lands in bin $i$* occurs, which is $\\frac{1}{n}$.\n",
    "\n",
    "\n",
    "Then we observe that $b_i= \\sum_{j=1}^n b_{i,j}$. Using this we can now compute the expected value easily:\n",
    "\n",
    "$$\\begin{align*} E[b_i] \n",
    "&= E\\left[ \\sum_{j=1}^n b_{i,j} \\right] \n",
    "\\\\\n",
    "&= \\sum_{j=1}^n E[b_{i,j}]\n",
    "\\\\\n",
    "&= \\sum_{j=1}^n \\frac{1}{n}\n",
    "\\\\\n",
    "&= n\\cdot \\frac{1}{n}\n",
    "\\\\\n",
    "&= 1\n",
    "\\end{align*}$$\n",
    "\n",
    "Once again we get 1. And once again this is fairly meaningless, as this does not say much about the distribution. For example if someone picked a random bin, and then threw all the balls into that bin, the expected bin size for any bin is 1, yet the distribution is horribly skewed. In particular, the expected value alone does not say anything about the third question \"Can you say something of the form: bin $i$ has size at most $x$ with probability at most $p$?\"\n",
    "\n",
    "## Markov\n",
    "\n",
    "To try to bound things better, we will use something known as Markov's inequality. To motivate it, suppose I have 100 students, and the average mark is 4. Then it is the case that less than half of the students can have marks above 8, and at most a quarter of students can have marks above 16. This simply follows by contradiction, and vitally uses the fact that marks are not negative, otherwise you can't conclude anything.\n",
    "\n",
    "**Markov's ineqality:** Given random value $X$ that is always a nonnegative value:\n",
    "$$ Pr[X> a ] \\leq \\frac{E[X]}{a}$$\n",
    "\n",
    "The proof follows directly by contradiction and the definition of expected value.\n",
    "\n",
    "Now, how does this apply to us? It allows us to give an answer to the third question. Expected value alone does not, but since the number of balls in bin $i$, $b_i$ is never negative, we can use Markov and our knowledge that $E[b_i]=\\frac{1}{n}$:\n",
    "\n",
    "$$Pr[b_i> x] < \\frac{E[b_i]}{x} = \\frac{1}{x}$$\n",
    "\n",
    "So this is more meaningful. We can say that a bin has less than 100 items 99% of this time.\n",
    "\n",
    "But, this bound is far from tight.\n",
    "\n",
    "Usually in a probability course you next learn Chebyshev's inequality, which is better than Markov when you know the variance of your random variable. Instead we will go straight to Chernoff bounds, which we will use several times.\n",
    "\n",
    "## Chernoff bounds\n",
    "\n",
    "Chernoff bounds provide excellent bounds on showing what the chance is that the outcome is close to the expected value. However, Chernoff bounds only work to bound a random variable that is the sum of independent random variables. \n",
    "\n",
    "Formally $X$ and $Y$ are independent if  for all $x,y$: $P[X=x \\text{ and } Y=y]=Pr[X=x]\\cdot Pr[Y=y]$. For example, $b_{i,j}$  and $b_{i,j'}$, $j \\not = j'$ are independent, this can be verified by looking at the probabilities of the four different outcomes of these two events, but informally whether ball $j$ ends up in bin $i$ has no effect on whether ball $j'$ ends up in bin $i$. On the other hand $b_{i,j}$ and $b_{i',j}$, $i \\not = i'$ are not independent! If the $j$th ball is in bin $i$ then it is certainly not in bin $i'$.\n",
    "\n",
    "There are many formulations of the statement of Chernoff bounds. One of the most usefull is\n",
    "\n",
    "\n",
    "**Chernoff bounds:** Let $X_1, X_2, \\ldots X_k$ be **independent** events with outcomes in the range $[0,1]$ and let $X=\\sum_{i=1}^k X_i$. Then:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "Pr[X \\leq (1-\\delta) E[X] ] & \\leq e^{-\\frac{\\delta^2E[X]}{2}}\n",
    "& \\text{For any $\\delta, 0\\leq \\delta \\leq 1$}\n",
    "\\\\\n",
    "Pr[X \\geq (1+\\delta) E[X] ] &\\leq e^{-\\frac{\\delta^2E[X]}{2+\\delta}}\n",
    "& \\text{For any $\\delta\\geq 0$}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, how does this apply to our hash tables? Well, we defined $b_i= \\sum_{j=1}^n b_{i,j}$, and we know the $b_{i,j}$'s are independent for different $j$'s. We also know $E[b_i]=1$. This gives us all of the needed ingredients. We will use the second formula, as we want to limit the chance of being above the expected value by too much. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq (1+\\delta) E[b_i]] & \\leq e^{-\\frac{\\delta^2E[X]}{2+\\delta}}\n",
    "\\\\\n",
    "Pr[b_i \\geq (1+\\delta)] & \\leq e^{-\\frac{\\delta^2}{2+\\delta}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, once again, we ask what is the chance that $b_i\\geq 100$? Markov bounded this at at most $1%$. To use Chernoff for this, set $\\delta=99$ and we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq 100 ]&\\leq e^{-\\frac{99^2}{2+99}} \n",
    "\\\\\n",
    "& = \\frac{1}{e^{\\frac{9801}{11}}}\n",
    "\\\\\n",
    "& \\approx 0.000000000000000000000000000000000000000000718\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So while Markov bounded 100 balls in a bin as a 1-in-100 event, small, but not impossible, Chernov says this will happen with a tiny probability. \n",
    "\n",
    "## With high probability\n",
    "\n",
    "If an event happens with probability $1-O(\\frac{1}{n^c})$, for some $c$ this is said to be with *polynomially high probability* (whp) Such an event has a chance of happening that goes to 1 as $n$ grows. This is stronger than saying something about a certain fixed percentage, such as 1%. \n",
    "\n",
    "(*Note that with high probability means any bound that tends to 1 as $n$ grows, but in practice this term is often used to mean that it goes with a polynomial and not something like $1-\\frac{1}{\\log \\log n}$*)\n",
    "\n",
    "Chernoff bounds are often used to make whp bounds. As we want to turn something like $e^{-\\delta}$ into a $n^-c$ it is natural to try setting $\\delta$ to a logarithm, since $e^{-c \\ln n}=n^c$.\n",
    "\n",
    "We can ask what is the chance that $b_i \\geq 1+\\alpha \\ln n $. Setting $\\delta= \\alpha \\ln n$, for some $\\alpha \\geq 1$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq (1+\\delta) & \\leq e^{-\\frac{\\delta^2}{2+\\delta}}\n",
    "\\\\\n",
    "Pr[b_i \\geq 1+\\alpha \\ln n] & \\leq e^{-\\frac{\\delta^2}{2+\\delta}}\n",
    "\\\\\n",
    "&= e^{-\\frac{\\alpha^2 \\ln^2 n}{2+\\alpha \\ln n}}\n",
    "\\\\\n",
    "&= e^{-\\alpha \\ln n}e^{\\frac{\\alpha \\ln n}{2+\\alpha \\ln n}}\n",
    "\\\\\n",
    "&= n^{-\\alpha}e^{\\frac{\\alpha \\ln n}{2+\\alpha \\ln n}}\n",
    "\\\\\n",
    "&= n^{-\\alpha}e & \\text{As $\\frac{\\alpha \\ln n}{2+\\alpha \\ln n}\\leq 1$}\n",
    "\\\\\n",
    "&= n^{-(\\alpha-1)} & \\text{When $n \\geq 3$}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we can conclude that $b_i$ is at most $1+2 \\ln n$ with probability $\\frac{1}{n}$, and $1+3 \\ln n$ with probability $\\frac{1}{n^2}$.\n",
    "This is often times stated $b_i$ is $O(\\ln n)$ whp.\n",
    "\n",
    "## Union bounds\n",
    "\n",
    "The Unions bound, also known a Boole'e inequality is a very loose bound on multiple event happening: \n",
    "\n",
    "$$P[X=x\\text{ and }Y=y] \\leq P[X=x] + P[Y=y]$$\n",
    "\n",
    "It often times gives useless bounds. For example, it says that if you flip 4 coins, the chance they are all heads is\n",
    "\n",
    "$$P[\\text{4 coins all heads}] \\leq 4 P[\\text{one coin heads}] = 4\\cdot\\frac{1}{2} =2$$\n",
    "\n",
    "Saying a probability is less than two is not saying anything!\n",
    "\n",
    "But, sometimes a union bound is exactly what is needed. Its main advantage is that it always works, you don't need to assume independence or anything else.\n",
    "\n",
    "We will use this to solve question 4: *Can you say something of the form: all bins have size at most $x$ with probability at most $p$?* \n",
    "\n",
    "We know that a single bin has size at most $1+3 \\ln n$ with probability $\\frac{1}{n^2}$. What about the probability that *all* bins have size at most $1+3 \\ln n$? This is asking $Pr[\\text{for all $i$ in $1..n$}, b_i \\leq 1+3 \\ln n]$. Using the union bound:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[\\text{for all $i$ in $1..n$}, b_i \\leq 1+3 \\ln n] \n",
    "& \\leq  \\sum_{i \\in 1..n} Pr[b_ \\leq 1+3 \\ln n]\n",
    "\\\\\n",
    "& \\leq  \\sum_{i \\in 1..n} \\frac{1}{n^2}\n",
    "\\\\\n",
    "& \\leq  n \\cdot \\frac{1}{n^2}\n",
    "\\\\\n",
    "& \\leq   \\frac{1}{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus not only is one bin have size $O(\\log n)$ whp, but all bins do!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate median finding\n",
    "\n",
    "I finished this in lecture 4, you can find the notes for this in that notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
